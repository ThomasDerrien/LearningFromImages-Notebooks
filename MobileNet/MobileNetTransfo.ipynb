{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6pF2ekwY-lq"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3llQEwkNZa0v",
        "outputId": "8aad63e9-3d32-4e63-b6c6-4f9c858cb348"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iz_1-uNZnAO",
        "outputId": "ef542669-21c6-49de-d50f-44af7d4d4fae"
      },
      "outputs": [],
      "source": [
        "!unzip /drive/MyDrive/data.zip -d ../\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jTz4N7lce-3",
        "outputId": "c5489a56-f0a6-4cae-9a0f-80d7f295373b"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOXRWGQuY-lx",
        "outputId": "c2f3bcd7-55ab-49bd-e7a6-6513a68f8aa1"
      },
      "outputs": [],
      "source": [
        "from os import path, listdir\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import models, transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module, Sequential, Conv2d, ReLU, MaxPool2d, LSTM, Linear\n",
        "from torch.optim import Adam\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xbzzNG2mY-l3"
      },
      "outputs": [],
      "source": [
        "IMAGE_HEIGHT = 128\n",
        "IMAGE_WIDTH = 256\n",
        "PLATE_SIZE = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFDLbLP1Y-l4"
      },
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJAI53MHcqBw",
        "outputId": "9970b3ac-a20a-471d-a7f0-c653ef7b3833"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHVQhiuI6Jmc"
      },
      "source": [
        "## Fonctions de preprocess\n",
        "\n",
        "Dans cette étape, nous prétraitons les images, c'est-à-dire que nous extrayons les plaques d'immatriculation grâce aux détections (fichier detections.csv, qui contient les coordonnées des plaques extraites par le modèle YOLO). Ensuite, nous redimensionnons les plaques afin qu'elles aient toutes la même taille."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XntlGJ3LY-l5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "from unidecode import unidecode\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def process_image(image_path, row, output_size):\n",
        "    try:\n",
        "        # Charger et découper l'image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            return None, None\n",
        "\n",
        "        x_min, x_max, y_min, y_max = map(int, [row[\"X_min\"], row[\"X_max\"], row[\"Y_min\"], row[\"Y_max\"]])\n",
        "        cropped = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        # Redimensionner avec transformations Torch\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(output_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x / 255.0 + 1e-3)\n",
        "        ])\n",
        "\n",
        "        processed_plate = transform(cropped)\n",
        "\n",
        "        # Extraire et transformer l'étiquette\n",
        "        plate = row[\"Plaque\"]\n",
        "        label = unidecode(plate).upper()\n",
        "\n",
        "        return processed_plate, label\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def extract_plates_and_labels(images_dir, csv_path, output_size):\n",
        "    # Précharger les données du CSV\n",
        "    detections = pd.read_csv(csv_path).set_index(\"Filename\")\n",
        "\n",
        "    plates, labels = [], []\n",
        "\n",
        "    # Filtrer les fichiers image\n",
        "    image_paths = [\n",
        "        os.path.join(images_dir, img_name)\n",
        "        for img_name in os.listdir(images_dir)\n",
        "        if img_name.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    ]\n",
        "\n",
        "    # Créer un dictionnaire des informations pour un accès plus rapide\n",
        "    detections_dict = detections.to_dict(orient='index')\n",
        "\n",
        "    def process_image_batch(image_path):\n",
        "        img_name = os.path.basename(image_path)\n",
        "        if img_name in detections_dict:\n",
        "            row = detections_dict[img_name]\n",
        "            return process_image(image_path, row, output_size)\n",
        "        return None, None\n",
        "\n",
        "    # Utiliser un ThreadPoolExecutor pour paralléliser le traitement des images\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(process_image_batch, image_paths), total=len(image_paths)))\n",
        "\n",
        "    # Ajouter les résultats dans les listes\n",
        "    for processed_plate, label in results:\n",
        "        if processed_plate is not None and label is not None:\n",
        "            plates.append(processed_plate)\n",
        "\n",
        "            labels.append(label)\n",
        "\n",
        "    return plates, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W6mC5oBLY-l6"
      },
      "outputs": [],
      "source": [
        "def encode_labels(labels, char_to_idx, padding_value=0, size=50):\n",
        "    encoded_labels = [[char_to_idx[char] for char in label] for label in labels]\n",
        "\n",
        "    # Pad each label to the max length\n",
        "    for i in range(len(encoded_labels)):\n",
        "        padding = [padding_value] * (size - len(encoded_labels[i]))\n",
        "        encoded_labels[i].extend(padding)\n",
        "\n",
        "    # Convert each list into a tensor\n",
        "    tensor_labels = [torch.tensor(label, dtype=torch.long) for label in encoded_labels]\n",
        "\n",
        "    return tensor_labels\n",
        "\n",
        "\n",
        "# Dictionnaires pour encoder\n",
        "char_to_idx = {\n",
        "            '': 0,\n",
        "            **{chr(i): idx for idx, i in enumerate(range(ord('A'), ord('Z') + 1), start=1)},\n",
        "            **{str(i): idx + 26 for idx, i in enumerate(range(10))},\n",
        "            '-': 37,\n",
        "            ' ': 38,\n",
        "            '.': 39\n",
        "        }\n",
        "idx_to_char = {v: k for k, v in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jwdPHzSJY-l8"
      },
      "outputs": [],
      "source": [
        "class PlateDataset(Dataset):\n",
        "    def __init__(self, images,labels):\n",
        "\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetch a single sample (image and label).\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, str]: The processed image tensor and its label.\n",
        "        \"\"\"\n",
        "        return (self.images[idx],self.labels[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nAFA266Y-l-",
        "outputId": "7b1a1098-4c0c-433e-a671-146a07adb647"
      },
      "outputs": [],
      "source": [
        "csv_path = \"/data/detections.csv\"\n",
        "\n",
        "X_train,Y_train = extract_plates_and_labels(\"/data/train\",csv_path,(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
        "X_val,Y_val =  extract_plates_and_labels(\"/data/val\",csv_path,(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
        "X_test,Y_test =  extract_plates_and_labels(\"/data/test\",csv_path,(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
        "\n",
        "Y_train_encoded = encode_labels(Y_train,char_to_idx,0,PLATE_SIZE)\n",
        "Y_val_encoded = encode_labels(Y_val,char_to_idx,0,PLATE_SIZE)\n",
        "Y_test_encoded = encode_labels(Y_test,char_to_idx,0,PLATE_SIZE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6oYWqvkY-mB",
        "outputId": "788cae16-b6be-46ca-bdb1-8ae7a32996da"
      },
      "outputs": [],
      "source": [
        "print(f\"NB train samples : {len(Y_train_encoded)}, X_train shape : {X_train[0].shape}\")\n",
        "print(f\"NB val samples : {len(Y_val_encoded)}, X_val shape : {X_val[0].shape}\")\n",
        "print(f\"NB test samples : {len(Y_test_encoded)}, X_test shape : {X_test[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtqYmTwYY-mC"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaWK0H347IL5"
      },
      "source": [
        "Model : actuellement un mobilenet pour extraire les features puis un transformer pour créer les séquences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hmDX-uqBY-mF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CRNNWithTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, plate_size, image_height, image_width,\n",
        "                 transformer_dim=64, num_heads=8, num_layers=2, dropout_rate=0.2):\n",
        "        super(CRNNWithTransformer, self).__init__()\n",
        "        self.plate_size = plate_size\n",
        "\n",
        "        # Use MobileNetV3 as feature extractor\n",
        "        mobilenet_v3 = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
        "        self.features = mobilenet_v3.features  # Extract convolutional layers only\n",
        "\n",
        "        OUTPUT_FEATURES = 960  # MobileNetV3 large outputs 960 channels in the last convolutional layer\n",
        "\n",
        "        # 1x1 Conv to match the desired feature dimensions\n",
        "        self.conv1x1 = nn.Conv2d(OUTPUT_FEATURES, 960, kernel_size=1)\n",
        "\n",
        "        # Compute convolutional output size dynamically\n",
        "        def calculate_conv_output_size(height, width):\n",
        "            dummy_input = torch.zeros(1, 3, height, width)\n",
        "            with torch.no_grad():\n",
        "                output = self.features(dummy_input)\n",
        "            return output.size(2), output.size(3)\n",
        "\n",
        "        conv_h, conv_w = calculate_conv_output_size(image_height, image_width)\n",
        "        self.feature_size = conv_h * conv_w * 960  # Adjusted based on MobileNetV3 output\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=transformer_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=transformer_dim * 2,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Projection from CNN feature map to transformer input dimension\n",
        "        self.feature_projection = nn.Linear(self.feature_size // plate_size, transformer_dim)\n",
        "\n",
        "        # Fully connected layer for class prediction\n",
        "        self.fc = nn.Linear(transformer_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = self.features(x)  # Shape: (batch_size, 960, conv_h, conv_w) from MobileNetV3\n",
        "        x = self.conv1x1(x)  # Shape: (batch_size, 960, conv_h, conv_w)\n",
        "\n",
        "        # Flatten and reshape for transformer\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()  # Shape: (batch_size, conv_h, conv_w, 1024)\n",
        "        x = x.view(batch_size, self.plate_size, -1)  # Shape: (batch_size, plate_size, feature_size // plate_size)\n",
        "        x = self.feature_projection(x)  # Project to transformer_dim\n",
        "\n",
        "        # Pass through Transformer Encoder\n",
        "        x = self.transformer(x)  # Shape: (batch_size, plate_size, transformer_dim)\n",
        "\n",
        "        # Class predictions\n",
        "        outputs = self.fc(x)  # Shape: (batch_size, plate_size, num_classes)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fKs8DkAfY-mG"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "# Créer le dataset et dataloader\n",
        "dataset_train = PlateDataset(X_train,Y_train_encoded)\n",
        "dataset_val = PlateDataset(X_val,Y_val_encoded)\n",
        "dataset_test = PlateDataset(X_test,Y_test_encoded)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4NAHnp9Y-mH",
        "outputId": "991a192e-8537-415d-e28e-f203d28ceb64"
      },
      "outputs": [],
      "source": [
        "model = CRNNWithTransformer(dropout_rate=0.3,num_classes=len(idx_to_char)+1,plate_size=PLATE_SIZE,image_height=IMAGE_HEIGHT,image_width=IMAGE_WIDTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJrN6DsxDThp",
        "outputId": "ee1174f4-d1d0-4f07-e692-671753367c9d"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-yjAFSJY-mH"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QqXWkh3pY-mI",
        "outputId": "7e8cf3c9-531d-4361-8974-8a23f1efdeda"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 100\n",
        "PATIENCE = 5  # Stop if validation loss doesn't improve for 5 epochs\n",
        "min_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_loss_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for images, labels in tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)  # Ensure labels are on the same device\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(images)  # (batch_size, sequence_length, num_classes)\n",
        "        output = output.permute(0, 2, 1)  # Change to (batch_size, num_classes, sequence_length)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(dataloader_train)\n",
        "    train_loss_values.append(epoch_train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader_val, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(images)\n",
        "            output = output.permute(0, 2, 1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(dataloader_val)\n",
        "    val_loss_values.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if epoch_val_loss < min_val_loss:\n",
        "        min_val_loss = epoch_val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
        "        print(f\"Validation loss improved. Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement in validation loss. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(\"Early stopping triggered. Training stopped.\")\n",
        "        break\n",
        "\n",
        "# Plot the loss graph for training and validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_loss_values) + 1), train_loss_values, label=\"Training Loss\")\n",
        "plt.plot(range(1, len(val_loss_values) + 1), val_loss_values, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(\"Training complete. Best model saved as 'best_model.pth'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_qwE7AZY-mJ"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUu_YGArkPav",
        "outputId": "0bbf6ef8-15a2-4de9-fedf-d7b1e42dba92"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()  # Mettre le modèle en mode évaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMUTsr07Y-mJ",
        "outputId": "aaeac617-3444-4c5e-98ca-ac02fbf46e56"
      },
      "outputs": [],
      "source": [
        "nbtot = 0\n",
        "nb_good = 0\n",
        "\n",
        "# Pour le calcul de l'accuracy par caractère\n",
        "total_characters = 0\n",
        "correct_characters = 0\n",
        "\n",
        "sequence_accuracies = []\n",
        "for images, labels in dataloader_test:\n",
        "    # Obtenir les prédictions du modèle\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(images)  # Output shape: (batch_size, sequence_length, num_classes)\n",
        "    predicted_labels = output.argmax(dim=2)  # Shape: (batch_size, sequence_length)\n",
        "\n",
        "    # Comparer les prédictions avec les étiquettes de vérité terrain\n",
        "    for i in range(len(predicted_labels)):\n",
        "        predicted_text = \"\".join([idx_to_char[idx] for idx in predicted_labels[i].detach().cpu().numpy()])\n",
        "        ground_truth = \"\".join([idx_to_char[idx] for idx in labels[i].detach().cpu().numpy()])\n",
        "\n",
        "        # Calcul de l'accuracy globale (match exact)\n",
        "        if ground_truth == predicted_text:\n",
        "            nb_good += 1\n",
        "        else :\n",
        "            print(f\"{ground_truth:^{40}} -- {predicted_text}\")\n",
        "        # Calcul de l'accuracy par caractère\n",
        "        correct_characters += sum(1 for a, b in zip(predicted_text, ground_truth) if a == b)\n",
        "        total_characters += len(ground_truth)\n",
        "\n",
        "        nbtot += 1\n",
        "\n",
        "# Calcul final des métriques\n",
        "exact_match_accuracy = nb_good / nbtot if nbtot > 0 else 0\n",
        "character_accuracy = correct_characters / total_characters if total_characters > 0 else 0\n",
        "\n",
        "print(f'Exact Match Accuracy: {exact_match_accuracy:.2%}')\n",
        "print(f'Character-level Accuracy: {character_accuracy:.2%}')\n",
        "torch.save(model.state_dict(), f'model_MobileNet_transfo_char={character_accuracy:.2%}_word={exact_match_accuracy:.2%}.pth')  # Save the best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ie_hJUg3JRbA",
        "outputId": "9d513712-c121-4bbf-f2fb-18122fa17880"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sélectionner aléatoirement 10 images\n",
        "sampled_images = []\n",
        "sampled_predictions = []\n",
        "sampled_ground_truths = []\n",
        "\n",
        "for images, labels in dataloader_test:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(images)  # Output shape: (batch_size, sequence_length, num_classes)\n",
        "    predicted_labels = output.argmax(dim=2)  # Shape: (batch_size, sequence_length)\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        if len(sampled_images) < 10:  # Collecter jusqu'à 10 échantillons\n",
        "            sampled_images.append(images[i].detach().cpu())\n",
        "            predicted_text = \"\".join([idx_to_char[idx] for idx in predicted_labels[i].detach().cpu().numpy()])\n",
        "            ground_truth = \"\".join([idx_to_char[idx] for idx in labels[i].detach().cpu().numpy()])\n",
        "            sampled_predictions.append(predicted_text)\n",
        "            sampled_ground_truths.append(ground_truth)\n",
        "        else:\n",
        "            break\n",
        "    if len(sampled_images) >= 10:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "for i,image in enumerate(sampled_images) :\n",
        "    image = image.permute(1, 2, 0) *255 # Permuter pour obtenir (H, W, C)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"GT: {sampled_ground_truths[i]}\\nPred: {sampled_predictions[i]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2c2dnWFz3U9",
        "outputId": "e2753116-c2d8-446b-dbf8-a14deb8c0068"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "letters = []\n",
        "\n",
        "for label in Y_train :\n",
        "    for char in label :\n",
        "            letters.append(char)\n",
        "# Compter les occurrences de chaque numéro\n",
        "label_counts = Counter(letters)\n",
        "\n",
        "# Afficher les résultats\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Caractère {label}: {count} occurrences\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
